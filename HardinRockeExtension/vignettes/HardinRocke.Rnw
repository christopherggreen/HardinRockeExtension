\documentclass[11pt]{article}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

%\VignetteIndexEntry{Replicating Table 1 of Hardin and Rocke (2005)}
%\VignetteDepends{HardinRockeExtension}
\SweaveOpts{prefix.string=hr, eps=FALSE, pdf=TRUE, strip.white=true}
\SweaveOpts{width=6, height=4.1}

%\usepackage{amsmath}
%\usepackage{amsfonts}% \mathbb
%\usepackage{mathtools}% -> \floor, \ceil
\usepackage[utf8]{inputenc}
%% The following is partly R's share/texmf/Rd.sty
\usepackage{color}
\usepackage{hyperref}
\definecolor{Blue}{rgb}{0,0,0.8}
\definecolor{Red}{rgb}{0.7,0,0}
\hypersetup{%
  hyperindex,%
  colorlinks={true},%
  pagebackref,%
  linktocpage,%
  plainpages={false},%
  linkcolor={Blue},%
  citecolor={Blue},%
  urlcolor={Red},%
  pdfstartview={XYZ null null 1},%
  pdfview={XYZ null null null},%
}

\usepackage{natbib}
\usepackage[noae]{Sweave}
%----------------------------------------------------
%\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
%\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
%\DeclareMathOperator{\sign}{sign}
%\newcommand{\abs}[1]{\left| #1 \right|}
%\newtheorem{definition}{Definition}
%\newcommand{\byDef}{\mathrm{by\ default}}
%\newcommand{\R}{{\normalfont\textsf{R}}{}}
%\newcommand{\texttt}[1]{\texttt{#1}}
%\newcommand*{\pkg}[1]{\texttt{#1}}
%\newcommand*{\CRANpkg}[1]{\href{http://CRAN.R-project.org/package=#1}{\pkg{#1}}}

%----------------------------------------------------
\begin{document}
%\setkeys{Gin}{width=0.9\textwidth}
%\setlength{\abovecaptionskip}{-5pt}

\title{Replicating Table 1 of Hardin and Rocke (2005)}
\author{Christopher~G. Green}
\maketitle
%\tableofcontents
<<init, echo=FALSE>>=
@

\section{Introduction}
This vignette shows how to use the function
\texttt{hrSimParallel} to replicate Table 1 of
\cite{HardinRocke:2005}, shown on page 942. 
We assume the reader can set up a cluster
using the \texttt{parallel} package, but this
is not necessary: one can create a cluster with
a single node.

Note that all code blocks in the vignette are marked
as ``do not evaluate'' to avoid running long simulations
during package checks. In the \texttt{R} code file 
resulting from running \texttt{Stangle} on the vignette
source, you will need to uncomment all the \texttt{R}
code to actually run the simulation.

\section{Setup}
First, we load some required packages and take care of some performance
tuning. The simulation requires the \texttt{CerioliOutlierDetection}
package as well as the \texttt{parallel} package. We also load the
\texttt{RhpcBLASctl} package to force the use of single-threaded BLAS,
if possible: with a multi-threaded BLAS on a multi-core machine, the
cluster ``nodes'' may impede each other. We want each worker to have
the node to itself for maximum efficiency.
<<setup, eval=FALSE>>=
# Replicate Table 1 in Hardin and Rocke (2005),
# page 942
#
# Christopher G. Green
# 2014-02-24
#
# run simulations in parallel

require( RhpcBLASctl             )
require( parallel                )
require( CerioliOutlierDetection )
require( HardinRockeExtension    )

# force single-threaded BLAS if possible
omp_set_num_threads(1)
@

Create a cluster using one of the \texttt{makeXXcluster}
functions from the \texttt{parallel} package. On Windows,
\texttt{makePSOCKcluster} is the only available option.
We assume 10 nodes here. \texttt{hrSimParallel} will 
distribute the work within a simulation run across 
the cluster.

The \texttt{useXDR} option should be set to FALSE
on little endian machines for efficiency.
<<makecluster, eval=FALSE>>=
thecluster <- makePSOCKcluster(10, 
	outfile="hr05_replicate.log", useXDR=FALSE)
@

We initialize the cluster random number generator
with a seed; this makes it possible for you to 
replicate our numbers and the resulting figures.
<<reproduce, eval=FALSE>>=
# make reproducible
clusterSetRNGStream(cl = thecluster, 2015)
@

Hardin and Rocke used 5000 simulation runs. We set 
a block size of 500 to manage the memory use on our
machine; this will allocate 500 runs to each cluster
node.
<<setsimsize, eval=FALSE>>=
N.SIM <- 5000
B.SIM <- 500
@

Now we initialize each cluster node using
\texttt{clusterEvalQ}. Each node needs a copy
of the two libraries we use for the simulation,
and we create a logfile for each node, tagged
with the process ID of the worker process running
on the node.
<<clusterinit, eval=FALSE>>=
# initialize each node
tmp.rv <- clusterEvalQ( cl = thecluster, {
  require( CerioliOutlierDetection )
  require( HardinRockeExtension    )
  require( mvtnorm                 )

  my.pid <- Sys.getpid()
  cat("My pid is ", my.pid, "\n")
  logfile <- paste("Test_Old_Method_Parallel_logfile_",my.pid,".txt",sep="")
  cat("Initialized\n\n", file=logfile)

  invisible(NULL)
})
@

Next, we generate the cases we will want to run. Each case
consists of a dimension $p$, a sample size $n$, and an MCD
fraction $\alpha$. Hardin and Rocke used $p \in \{5, 10, 20\}$
and $n \in \{50,100,500,1000\}$, and assumed $\alpha$ was
equal to the maximum breakdown point fraction 
$\frac{\lfloor (n+p+1)/2 \rfloor}{n}$. We run some additional
MCD fractions here to illustrate how well the Hardin-Rocke
method works outside of its design parameters.

We chose to order the cases by decreasing sample size so that
the most ``expensive'' cases would run first on our cluster; this
is not required.

Finally, we rotate the matrix of cases to a data frame for use
with lapply and variants (recall that a data frame is a list of
its columns).
<<buildcases, eval=FALSE>>=
# want each case to be a column so that we can use parLapply
hr.cases <- as.matrix(expand.grid(list(p=c(5,10,20),n=c(50,100,500,1000),
  mcd.alpha=c(0.65,0.75,0.85,0.95,0.99))))
hr.cases <- rbind(cbind(hr.cases,FALSE),
  t(apply(as.matrix(expand.grid(list(p=c(5,10,20),n=c(50,100,500,1000)))),
    1,function(x) c(x,mcd.alpha=floor( (x[2] + x[1] + 1)/2 )/x[2],mbp=TRUE) )))
hr.cases <- unique(hr.cases)
hr.cases <- hr.cases[ order(hr.cases[,"mcd.alpha"]),]
hr.cases <- hr.cases[ order(hr.cases[,"n"],decreasing=TRUE),]
dimnames(hr.cases)[[2]] <- c("p","n","mcd.alpha","mbp")
hr.cases <- data.frame(t(hr.cases))
@

\section{Simulation Runs}
Now we run the simulation. We distribute the work of each
case across the cluster. Empirical testing showed that this
was faster on our machines than running entire cases in 
parallel.

Remember to stop your cluster when you're done.
<<runsim, eval=FALSE>>=
cat("Starting run at ", format(Sys.time()), "\n")

hrResults <- lapply(hr.cases, function(pn,clst,ns,bs) {
    cat("Trial p = ",pn[1]," n = ",pn[2]," mcd.alpha = ",pn[3],"\n")
    hrSimParallel(cl=clst, p = pn[1] , n = pn[2], 
      mcd.alpha=pn[3], alpha=0.05, N=ns, B=bs, lgf=logfile)
  }, clst=thecluster, ns=N.SIM, bs=B.SIM
)

cat("Run completed at ", format(Sys.time()), "\n")
stopCluster(thecluster)
# it is a good idea to save hrResults to a file
# here!!!
# save(hrResults, file="hrResults.rda")
@

\section{Analysis of Simulation Results}
Now we calculate and store the mean and standard deviation
of the results for each case.
<<calcstats, eval=FALSE>>=
allmeans <- as.data.frame(t(rbind( hr.cases, 
  100*sapply( hrResults, function(x) colMeans(x) )
)))
row.names(allmeans) <- NULL
# format n and mcd.alpha so that reshape looks nicer
allmeans$n <- factor(sprintf("%04d", allmeans$n), ordered=TRUE)
allmeans$mcd.alpha <- sprintf("%0.3f", allmeans$mcd.alpha)
# collapse all the maximum breakdown point cases to one
# since the MBP alpha value differs for different n and p
allmeans$mcd.alpha[ allmeans$mbp==1 ] <- "MBP"
allmeans$mcd.alpha <- factor(allmeans$mcd.alpha)

allstds <- as.data.frame(t(rbind( hr.cases, 
  100*sapply( hrResults, function(x) apply(x,2,sd) )
)))
row.names(allstds) <- NULL
# format n and mcd.alpha so that reshape looks nicer
allstds$n <- factor(sprintf("%04d", allstds$n), ordered=TRUE)
allstds$mcd.alpha <- sprintf("%0.3f", allstds$mcd.alpha)
# collapse all the maximum breakdown point cases to one
# since the MBP alpha value differs for different n and p
allstds$mcd.alpha[ allstds$mbp==1 ] <- "MBP"
allstds$mcd.alpha <- factor(allstds$mcd.alpha)
@

Finally, we reshape the results for easier comparison
to Table 1 of \cite{HardinRocke:2005}. First, here are
the maximum breakdown point cases; compare to the 
Hardin and Rocke paper's results.
<<tabularize1, eval=FALSE>>=
# format for easier comparision to hardin and rocke paper
# reorder columns so that n increases as we move left to
# right
cat("Means:\n")
print(reshape(allmeans.mbp[,c("p","n","CHI2.CON")  ],
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])
print(reshape(allmeans.mbp[,c("p","n","HRASY.CON") ], 
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])
print(reshape(allmeans.mbp[,c("p","n","HRPRED.CON")], 
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])

cat("Standard deviations:\n")
print(reshape(allstds.mbp[,c("p","n","CHI2.CON")   ],
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])
print(reshape(allstds.mbp[,c("p","n","HRASY.CON")  ],
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])
print(reshape(allstds.mbp[,c("p","n","HRPRED.CON") ], 
  direction="wide", idvar=c("p"),timevar="n")[,c(1,5:2)])
@

Next, we can expand these tables to look at the behavior
of the tests we use other values for the MCD fraction.
<<tabularize2, eval=FALSE>>=
# format for easier comparision to hardin and rocke paper
cat("Means:\n")
print(reshape(allmeans[,c("p","n","mcd.alpha","CHI2.CON")  ],
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])
print(reshape(allmeans[,c("p","n","mcd.alpha","HRASY.CON") ], 
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])
print(reshape(allmeans[,c("p","n","mcd.alpha","HRPRED.CON")], 
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])

cat("Standard deviations:\n")
print(reshape(allstds[,c("p","n","mcd.alpha","CHI2.CON")   ],
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])
print(reshape(allstds[,c("p","n","mcd.alpha","HRASY.CON")  ],
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])
print(reshape(allstds[,c("p","n","mcd.alpha","HRPRED.CON") ], 
  direction="wide", idvar=c("mcd.alpha","p"),timevar="n")[,c(1:2,6:3)])
@

Tables 2 and 3 of \cite{HardinRocke:2005} use an identical set 
up but a different significance level. Those tables can be reproduced
by setting \texttt{alpha} to a different value.
\bibliographystyle{plainnat}
\bibliography{HardinRockeExtension}
\end{document}
